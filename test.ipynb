{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starter.py content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pylab as plb\n",
    "import numpy as np\n",
    "import mountaincar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FROM Project 1!\n",
    "\n",
    "# To calculate the gaussian\n",
    "def gauss(x,p):\n",
    "    \"\"\"Return the gauss function N(x), with mean p[0] and std p[1].\n",
    "    Normalized such that N(x=p[0]) = 1.\n",
    "    \"\"\"\n",
    "    return np.exp((-(x - p[0])**2) / (2 * p[1]**2))\n",
    "\n",
    "# To calculate the distance between two vectors\n",
    "def distance(vec1,vec2):\n",
    "    assert np.shape(vec1)==np.shape(vec2)\n",
    "    #print(\"   Distance between\",vec1,\"and\",vec2)\n",
    "    term1 = np.power(vec1-vec2,2)\n",
    "    #print(\"      term1 :\",term1)\n",
    "    term2 = np.sqrt(np.sum(term1,axis=1))\n",
    "    #print(\"      term2 :\",term2)\n",
    "    return term2\n",
    "\n",
    "# To assign data to the closest center\n",
    "def assign_cluster(centers, datas):\n",
    "    #initialization\n",
    "    cluster_assignment = np.zeros(np.shape(datas)[0]) - 1\n",
    "    current_c = np.tile(centers[0], (np.shape(datas)[0],1))\n",
    "    eucl_distance = distance(current_c,datas)\n",
    "    min_distance = np.copy(eucl_distance)\n",
    "    #find cluster for each entry in datas\n",
    "    for i in range(0,np.shape(centers)[0]):\n",
    "        #print(\"Center is : \",centers[i], \"iteration : \",i)\n",
    "        current_c = np.tile(centers[i], (np.shape(datas)[0],1))\n",
    "        eucl_distance = distance(current_c,datas)\n",
    "        #print(\"   MD :\",min_distance)\n",
    "        #print(\"   ED :\",eucl_distance)\n",
    "        bool_distance = eucl_distance <= min_distance\n",
    "        #print(\"   BD :\",bool_distance)\n",
    "        min_distance = eucl_distance*(bool_distance) + min_distance*(1 - bool_distance)\n",
    "        #print(\"   MD :\",min_distance)        \n",
    "        cluster_assignment = cluster_assignment*(1 - bool_distance) + i*bool_distance\n",
    "        #print(\"   CA :\",cluster_assignment)\n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DummyAgent():\n",
    "    \"\"\"A not so good agent for the mountain-car task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mountain_car = None, parameter1 = 3.0):\n",
    "        \n",
    "        if mountain_car is None:\n",
    "            self.mountain_car = mountaincar.MountainCar()\n",
    "        else:\n",
    "            self.mountain_car = mountain_car\n",
    "\n",
    "        self.parameter1 = parameter1\n",
    "\n",
    "    def visualize_trial(self, n_steps = 200):\n",
    "        \"\"\"Do a trial without learning, with display.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps -- number of steps to simulate for\n",
    "        \"\"\"\n",
    "        \n",
    "        # prepare for the visualization\n",
    "        plb.ion()\n",
    "        mv = mountaincar.MountainCarViewer(self.mountain_car)\n",
    "        mv.create_figure(n_steps, n_steps)\n",
    "        plb.draw()\n",
    "            \n",
    "        # make sure the mountain-car is reset\n",
    "        self.mountain_car.reset()\n",
    "\n",
    "        for n in range(n_steps):\n",
    "            print ('\\rt =', self.mountain_car.t,\n",
    "            sys.stdout.flush())\n",
    "            \n",
    "            # choose a random action\n",
    "            self.mountain_car.apply_force(np.random.randint(3) - 1)\n",
    "            # simulate the timestep\n",
    "            self.mountain_car.simulate_timesteps(100, 0.01) #MEANS simulation EVERY second of the state of the car !!!\n",
    "\n",
    "            # update the visualization\n",
    "            mv.update_figure()\n",
    "            plb.draw()            \n",
    "            \n",
    "            # check for rewards\n",
    "            if self.mountain_car.R > 0.0:\n",
    "                print (\"\\rreward obtained at t = \", self.mountain_car.t)\n",
    "                break\n",
    "    \n",
    "    def learn(self):\n",
    "        # This is your job!\n",
    "        \n",
    "        # Implement Sarsa Lambda: Reinforcement Learning Algo\n",
    "        # TO DO: http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html\n",
    "        #\n",
    "        # Initialize Q(s,a) arbitrarily\n",
    "        # Repeat (for each episode)\n",
    "        #   Initialize s\n",
    "        #   Choose a from s using policy derived from Q (e.g. Espylon greedy)\n",
    "        #   Repeat (for each step of episode)\n",
    "        #     Take action a, observe r, s'\n",
    "        #     Choose a' from s' using policy derived from Q (e.g. Epsylon greedy)\n",
    "        #     Q(s,a) <-- Q(s,a) + Alpha(r + Gamma*Q(s',a') - Q(s,a));\n",
    "        #     s <-- s';      a <-- a';\n",
    "        #   until s is terminal\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My work start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INITIALIZATION\n",
    "\n",
    "#Just to import the inner value of the class MountainCar such as x, x_d etc.\n",
    "mountain_car = mountaincar.MountainCar()\n",
    "\n",
    "# For the call of Simulate Time Step\n",
    "dt=0.01; # Time of each step\n",
    "n_steps=100; # Number of timesteps\n",
    "\n",
    "#Number of time the trial is repeated\n",
    "n_trials=10;\n",
    "\n",
    "#Number of Neurons for the grid (nNeurons X nNeurons)\n",
    "nNeurons=20;\n",
    "\n",
    "#Variable for Neuron's Grid Initialization\n",
    "x_min=-150; #position\n",
    "x_max=30; #position\n",
    "xPoint_min=-15; #velocity\n",
    "xPoint_max=15; #velocity\n",
    "pos=np.linspace(x_min,x_max,nNeurons); #repartition of neuron through the grid (position)\n",
    "vel=np.linspace(xPoint_min,xPoint_max,nNeurons); #repartition of neuron through the grid (velocity)\n",
    "pos_c=np.ones((nNeurons))*pos; #Initialize centers with reparted value (position)\n",
    "vel_c=np.ones((nNeurons))*vel; #Initialize centers with reparted value (velocity)\n",
    "\n",
    "#Number of Actions possible at each step: Left, Right or Nothing\n",
    "Actions=3; \n",
    "\n",
    "#Reward Factor (gamma)\n",
    "Gamma=0.95;\n",
    "\n",
    "#Eligibility Decay rate (lambda)\n",
    "Lambda=0.5; #Can be comprised between 0 and 1\n",
    "\n",
    "#Learning Rate (eta)\n",
    "Eta=0.1 #LR<<1\n",
    "\n",
    "#Neuron Activity Matrix\n",
    "r=np.zeros((nNeurons,nNeurons));\n",
    "\n",
    "#Width of Gaussian which is equal to the space between each center\n",
    "sigma_x=abs(abs(x_max)+abs(x_min))/nNeurons; #space between each center on the position axis\n",
    "sigma_xPoint=abs(abs(xPoint_max)+abs(xPoint_min))/nNeurons; #space between each center on the velocity axis\n",
    "\n",
    "# Initializing weights and eligibility to zero\n",
    "w=np.zeros((nNeurons,nNeurons,Actions));    #DIM 20x20x3 \n",
    "e=np.zeros((nNeurons,nNeurons,Actions));    #DIM 20x20x3 \n",
    "\n",
    "#Initilizing Q(s,a)\n",
    "Q=np.zeros((nNeurons,nNeurons,Actions));    #DIM 20x20x3   \n",
    "\n",
    "#Parameters Tau\n",
    "Tau=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recalculate weight \n",
    "# AT EACH TIME STEP!\n",
    "cond=0;\n",
    "\n",
    "for i in range(0,n_trials):\n",
    "    # Initializiing e, s and a to 0\n",
    "    state=np.zeros((nNeurons,nNeurons)); # Dim of STATE: Pos X Vel\n",
    "    actions=np.zeros((1,Actions)); # Actions = 3 --> \"left\", \"right\", \"none\".\n",
    "    e=np.zeros((Actions, nNeurons, nNeurons)); # e depend of s and : Actions X Pos X Vel                \n",
    "\n",
    "    # For each TIME STEP: (of one experiment)\n",
    "    while(cond<=0):\n",
    "        mountain_car.simulate_timesteps(n_steps, dt)\n",
    "        # Computing Activity r_j(s) \n",
    "        ################################################################################\n",
    "        for X in range(0,nNeurons): # To go trhough position\n",
    "            for XD in range(0,nNeurons): # To go through velocities\n",
    "                r[X,XD]=np.exp(- pow(((pos_c[X]-mountain_car.x)/sigma_x),2) - pow(((vel_c[XD]-mountain_car.x_d)/sigma_xPoint),2));\n",
    "        \n",
    "        for X in range(0,nNeurons): # To go trhough position\n",
    "            for XD in range(0,nNeurons): # To go through velocities\n",
    "                for A in range(0,Actions): # To go through actions\n",
    "                    #Calculte Q!!!\n",
    "                    Q[X,XD,A]=w[X,XD,A]*r[X,XD]; \n",
    "        \n",
    "        for A in range(0,Actions):\n",
    "            Q(A)=np.sum(Q[:,:,0],(0,1)); \n",
    "            \n",
    "        SUM_Q=np.sum(Q);\n",
    "        \n",
    "        p_test=0;\n",
    "        for A in range(0,Actions):\n",
    "            P(A)=np.exp(Q(A)/Tau)/(SUM_Q/Tau);\n",
    "            if(P(A)>p_test):\n",
    "                p_test=P(A);\n",
    "                index=A;\n",
    "            \n",
    "        #REDO P(a*=a) in for s' instead of s to calculate Q(s',a')\n",
    "        \n",
    "            #Define new State regarding the action we will take\n",
    "            #Recalculate r_j(s) with \"_simulate_single_timestep(self, dt)\" \n",
    "            #--> get value x and x_d to recalculate Q(s',a') with r_j(s), keep same weight.\n",
    "            # Créer fonction check_reward (check si x>0) --> return r=0 ou r=1.\n",
    "        \n",
    "        #Calcul Delta_t\n",
    "            #Delta_t=r_(t+1) - [Q(s,a) - Gamma*Q(s',a')]\n",
    "        \n",
    "        #Calcul Eligibility\n",
    "        \n",
    "        #Update Weights\n",
    "            #NewWeights=OldWeights+LearningRate*Delta_t*Eligibility\n",
    "        \n",
    "        \n",
    "\n",
    "        #WHILE EXIT CONDITION\n",
    "        if(mountain_car.x>0):\n",
    "            cond=1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa Algo (below) from: https://github.com/studywolf/blog/blob/master/RL/SARSA%20vs%20Qlearn%20cliff/sarsa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Sarsa:\n",
    "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        self.q = {}\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward \n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2, action2):\n",
    "        qnext = self.getQ(state2, action2)\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma * qnext)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
